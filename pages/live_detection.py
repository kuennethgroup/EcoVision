# pages/live_detection.py
import streamlit as st
import cv2
import numpy as np
from streamlit_webrtc import webrtc_streamer, RTCConfiguration, VideoProcessorBase
import av
import threading
from PIL import Image
import io
import time

from src.logic.image_processing import process_image
from src.logic.emissions_calculator import calculate_emissions, export_data

RTC_CONFIGURATION = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})

def live_detection_page(model, confidence, class_filter, co2_data, detection_type):
    # Display detection type info with enhanced weight estimation method
    st.header("Capture and Process")
    
    # Show detection type specific info with enhanced processing details
    if detection_type == "Segmentation":
        st.info("üéØ **Enhanced Segmentation Mode**: Using DIVESPOT-inspired 3D point cloud analysis for precise weight estimation")
        st.markdown("""
        **Enhanced Weight Estimation Process:**
        - üîç **Segmentation**: Pixel-level food boundary detection
        - üìè **Depth Analysis**: Depth-Anything-V2-Small-hf model (0.2-3.0m range)
        - üé• **Camera Calibration**: Dynamic intrinsic parameter calculation
        - ‚òÅÔ∏è **Point Cloud Generation**: 3D point cloud from depth + camera intrinsics
        - üìê **Volume Estimation**: Multiple methods (Convex Hull, Voxel Grid, Alpha Shape)
        - ‚öñÔ∏è **Weight Calculation**: Volume √ó Food Density with robust median filtering
        - üå± **CO‚ÇÇ Calculation**: Enhanced Weight √ó CO‚ÇÇ Factor
        """)
    else:
        st.info("üì¶ **Standard Detection Mode**: Using average weights for quick estimation")
    
    handle_capture_webrtc_process(model, confidence, class_filter, co2_data, detection_type)

def handle_capture_webrtc_process(model, confidence, class_filter, co2_data, detection_type):
    st.subheader("üìπ Camera Capture")

    class CaptureVideoProcessor(VideoProcessorBase):
        def __init__(self):
            self.lock = threading.Lock()
            self.captured_frame = None
            self.capture_requested = False

        def recv(self, frame):
            img = frame.to_ndarray(format="bgr24")

            with self.lock:
                if self.capture_requested:
                    self.captured_frame = img.copy()
                    st.session_state.webrtc_emissions['captured_frame'] = img.copy()
                    self.capture_requested = False

            return av.VideoFrame.from_ndarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), format="rgb24")

        def request_capture(self):
            with self.lock:
                self.capture_requested = True

        def get_captured_frame(self):
            with self.lock:
                return self.captured_frame.copy() if self.captured_frame is not None else None

    webrtc_ctx = webrtc_streamer(
        key="simple_capture",
        video_processor_factory=CaptureVideoProcessor,
        rtc_configuration=RTC_CONFIGURATION,
        media_stream_constraints={"video": True, "audio": False},
    )

    if webrtc_ctx.state.playing:
        st.success("‚úÖ Camera is active - ready to capture!")

        if st.button("üì∏ Capture Frame", key="simple_capture_btn", type="primary"):
            if webrtc_ctx.video_processor:
                webrtc_ctx.video_processor.request_capture()
                st.success("üì∏ Frame captured! Processing...")
                time.sleep(0.5)
                captured_frame = webrtc_ctx.video_processor.get_captured_frame()
                if captured_frame is not None:
                    st.session_state.webrtc_emissions['captured_frame'] = captured_frame
                st.rerun()
            else:
                st.error("‚ùå Video processor not available")

    if st.session_state.webrtc_emissions.get('captured_frame') is not None:
        st.markdown("---")
        st.subheader("üî¨ Processing Captured Frame")

        frame = st.session_state.webrtc_emissions['captured_frame']

        # Show different processing messages based on detection type
        if detection_type == "Segmentation":
            processing_message = "üß† Processing with DIVESPOT-inspired 3D volume estimation..."
        else:
            processing_message = "‚ö° Processing with standard detection and average weights..."
            
        with st.spinner(processing_message):
            results, output = process_image(frame, model, confidence, class_filter, co2_data, detection_type)

        if results is not None and output is not None:
            col1, col2 = st.columns(2)

            with col1:
                st.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), caption="üì∏ Captured Frame", use_column_width=True)

            with col2:
                st.image(output, caption="üîç Processed Result", use_column_width=True)

            emissions_df, total_co2 = calculate_emissions(results, model, co2_data)

            if not emissions_df.empty:
                st.subheader("üå± Carbon Emissions Report")
                
                # Display method-specific information
                if detection_type == "Segmentation":
                    st.success("‚ú® **Enhanced Analysis**: Weight calculated using DIVESPOT-inspired 3D point cloud volume estimation")
                    
                    # Show enhanced weight estimation details if available
                    if hasattr(results, 'estimated_weights') and results.estimated_weights:
                        individual_weights = results.estimated_weights.get('_individual_weights', {})
                        if individual_weights:
                            st.markdown("**Enhanced Weight Estimation Details:**")
                            for mask_id, info in individual_weights.items():
                                st.markdown(f"‚Ä¢ **{mask_id}**: {info['weight_g']:.1f}g (vol: {info['volume_cm3']:.1f}cm¬≥, 3D points: {info['points_3d_count']}, methods: {info['methods_used']})")
                        
                        st.markdown("**Total Weight per Class:**")
                        for food, info in results.estimated_weights.items():
                            if food != '_individual_weights':
                                st.markdown(f"‚Ä¢ **{food}**: {info['total_weight_g']:.1f}g total from {info['count']} mask(s) (avg vol: {info['avg_volume_cm3']:.1f}cm¬≥)")
                else:
                    st.info("üìä **Standard Analysis**: Weight estimated using database average weights")
                
                # Format dataframe differently based on detection type
                if detection_type == "Segmentation":
                    # For segmentation: no Unit Weight column
                    formatted_columns = {
                        'Total Weight (kg)': '{:.3f}',
                        'CO2 emissions (kg)- class': '{:.3f}',
                        'CO‚ÇÇ Emissions (kg)': '{:.3f}'
                    }
                else:
                    # For detection: include Unit Weight column
                    formatted_columns = {
                        'Unit Weight (kg)': '{:.3f}',
                        'Total Weight (kg)': '{:.3f}',
                        'CO2 emissions (kg)- class': '{:.3f}',
                        'CO‚ÇÇ Emissions (kg)': '{:.3f}'
                    }
                
                st.dataframe(
                    emissions_df.style.format(formatted_columns),
                    use_container_width=True
                )

                st.metric("Total CO‚ÇÇ Impact", f"{total_co2:.2f} kg CO‚ÇÇ eq")

                st.markdown("### üì• Export Results")
                export_data(emissions_df, detection_type)

                buf = io.BytesIO()
                Image.fromarray(output).save(buf, format="PNG")
                st.download_button("üì• Download Processed Image", buf.getvalue(), "processed_image.png", "image/png")
            else:
                st.info("‚ÑπÔ∏è No food items detected in the captured frame")
        else:
            st.error("‚ùå Failed to process the captured frame. Please try again.")

        if st.button("üîÑ Capture New Frame", key="reset_simple_capture"):
            st.session_state.webrtc_emissions['captured_frame'] = None
            st.rerun()
